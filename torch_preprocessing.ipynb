{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EamfpV5v6bteYcknWwAQvO2Fai29Xk2e",
      "authorship_tag": "ABX9TyNbfzfZhb0ZGzfR0VpOkOwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10Academy-Group-4/Week-4/blob/mdahwireng_preprocessing/torch_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDwR7JKg95qs"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fROLFu1nTGw",
        "outputId": "12e5fb76-1de7-48cf-fe91-77cdb55d1594"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/MyDrive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/MyDrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II2IbHRfpBse",
        "outputId": "eeb3b633-9f4b-4d96-9e03-7530c946efa6"
      },
      "source": [
        "%cd /content/drive/MyDrive/MyDrive/10acad/week4_speech_to_text/ALFFA_PUBLIC/ASR/SWAHILI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MyDrive/10acad/week4_speech_to_text/ALFFA_PUBLIC/ASR/SWAHILI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86te444t0E1"
      },
      "source": [
        "import os\n",
        "os.chdir(\"data/train/wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN9zi1hp1Iuj"
      },
      "source": [
        "train_folders = os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGbew0iouAbT"
      },
      "source": [
        "waves =[]\n",
        "for folder in train_folders:\n",
        "  for f in os.listdir(folder):\n",
        "    if f.endswith('.wav'):\n",
        "      waves.append(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx-FFPuD94ML"
      },
      "source": [
        "os.chdir(\"..\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvIRG3V9-S8r"
      },
      "source": [
        "os.listdir()\n",
        "transcript = pd.read_csv('text', sep='\\t', names=['file','transcript'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "feIzLLN3-6is",
        "outputId": "3c4afcd4-ffc4-498f-cad6-c55848149422"
      },
      "source": [
        "transcript.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
              "      <td>rais wa tanzania jakaya mrisho kikwete</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
              "      <td>yanayo andaliwa nami pendo pondo idhaa ya kisw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
              "      <td>inayokutangazia moja kwa moja kutoka jijini da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
              "      <td>juma hili bara la afrika limeshuhudia raia wa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SWH-05-20101106_16k-emission_swahili_05h30_-_0...</td>\n",
              "      <td>wakipiga kura ya maoni ilikufanya mabadiliko ya</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                file                                         transcript\n",
              "0  SWH-05-20101106_16k-emission_swahili_05h30_-_0...             rais wa tanzania jakaya mrisho kikwete\n",
              "1  SWH-05-20101106_16k-emission_swahili_05h30_-_0...  yanayo andaliwa nami pendo pondo idhaa ya kisw...\n",
              "2  SWH-05-20101106_16k-emission_swahili_05h30_-_0...  inayokutangazia moja kwa moja kutoka jijini da...\n",
              "3  SWH-05-20101106_16k-emission_swahili_05h30_-_0...  juma hili bara la afrika limeshuhudia raia wa ...\n",
              "4  SWH-05-20101106_16k-emission_swahili_05h30_-_0...    wakipiga kura ya maoni ilikufanya mabadiliko ya"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APArjKVWASgv",
        "outputId": "bb599de9-a70a-4d1a-e7e4-06e148a9117f"
      },
      "source": [
        "pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbI0Iv4z_-r6"
      },
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from IPython.display import Audio\n",
        "\n",
        "class PrepSound():\n",
        "  # ----------------------------\n",
        "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
        "  # ----------------------------\n",
        "  def __init__(self, audio_file, path):\n",
        "    self.audio = audio_file\n",
        "    self.aud_path = path + '/' + audio_file\n",
        "  \n",
        "\n",
        "  def open(self, output=False):\n",
        "    samples, sr = torchaudio.load(self.aud_path)\n",
        "    self.samples = samples\n",
        "    self.sample_rate = sr\n",
        "    if output:\n",
        "      return (samples, sr)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert the given audio to the desired number of channels\n",
        "  # ----------------------------\n",
        "  def rechannel(self, new_channel, output=False):\n",
        "    samples = self.samples\n",
        "    sr = self.sample_rate\n",
        "\n",
        "    if (samples.shape[0] == new_channel):\n",
        "      pass\n",
        "\n",
        "    if (new_channel == 1):\n",
        "      # Convert from stereo to mono by selecting only the first channel\n",
        "      samples = samples[:1, :]\n",
        "      self.samples = samples\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      self.samples = torch.cat([samples, samples])\n",
        "      self.samples = samples\n",
        "    if output:\n",
        "      return ((samples, sr))\n",
        "\n",
        "  # ----------------------------\n",
        "  # Since Resample applies to a single channel, we resample one channel at a time\n",
        "  # ----------------------------\n",
        "  \n",
        "  def resample(self,newsr, output=False):\n",
        "    samples = self.samples\n",
        "    sr = self.sample_rate\n",
        "\n",
        "    if (sr == newsr):\n",
        "      pass\n",
        "\n",
        "    num_channels = samples.shape[0]\n",
        "    # Resample first channel\n",
        "    resamp = torchaudio.transforms.Resample(sr, newsr)(samples[:1,:])\n",
        "    if (num_channels > 1):\n",
        "      # Resample the second channel and merge both channels\n",
        "      resamp_dup = torchaudio.transforms.Resample(sr, newsr)(samples[1:,:])\n",
        "      resamp = torch.cat([resamp, resamp_dup])\n",
        "    self.samples = resamp\n",
        "    self.sample_rate = newsr\n",
        "    if output:\n",
        "      return ((resamp, newsr))\n",
        "    \n",
        "\n",
        "  # ----------------------------\n",
        "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "  # ----------------------------\n",
        "  def pad_trunc(self, max_ms, output=False):\n",
        "    samples = self.samples\n",
        "    sr = self.sample_rate\n",
        "    num_rows, samples_len = samples.shape\n",
        "    max_len = sr//1000 * max_ms\n",
        "\n",
        "    if (samples_len > max_len):\n",
        "      # Truncate the signal to the given length\n",
        "      samples = samples[:,:max_len]\n",
        "\n",
        "    elif (samples_len < max_len):\n",
        "      # Length of padding to add at the beginning and end of the signal\n",
        "      pad_begin_len = random.randint(0, max_len - samples_len)\n",
        "      pad_end_len = max_len - samples_len - pad_begin_len\n",
        "\n",
        "      # Pad with 0s\n",
        "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "      samples = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "      \n",
        "    self.samples = samples\n",
        "    self.sample_rate = sr\n",
        "    if output:\n",
        "      return ((samples, sr))\n",
        "\n",
        "  # ----------------------------\n",
        "  # Generate a Spectrogram\n",
        "  # ----------------------------\n",
        "  def spectro_gram(self, n_mels=64, n_fft=1024, hop_len=None, output=False):\n",
        "    samples = self.samples\n",
        "    sr = self.sample_rate\n",
        "    top_db = 80\n",
        "\n",
        "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
        "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(samples)\n",
        "\n",
        "    # Convert to decibels\n",
        "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "    if output:\n",
        "      return (spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "kEAit_xyBKHn",
        "outputId": "f5b8b95b-7bc8-43a3-c13e-eeb5db56b94e"
      },
      "source": [
        "# ----------------------------\n",
        "  # Convert the given audio to the desired number of channels\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def rechannel(aud, new_channel):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sig.shape[0] == new_channel):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    if (new_channel == 1):\n",
        "      # Convert from stereo to mono by selecting only the first channel\n",
        "      resig = sig[:1, :]\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      resig = torch.cat([sig, sig])\n",
        "\n",
        "    return ((resig, sr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-feaf5401c30c>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    @staticmethod\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCG7lSWpikjz"
      },
      "source": [
        "##**Load audio file**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9ojedwiuHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ3udaAotjHQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE5amNUJivnz"
      },
      "source": [
        "##**Load transcriptions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nADL4Zvi7mH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDP0Hw7di8Ea"
      },
      "source": [
        "##**Convert into channels** \n",
        "Some of the sound files are mono (ie. 1 audio channel) while most of them are stereo (ie. 2 audio channels). Since the Neural network model expects all items to have the same dimensions, we will convert the mono files to stereo, by duplicating the first channel to the second\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eclp7dxDjVzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NjiB1XjXQK"
      },
      "source": [
        "##**Standardize sampling rate**\n",
        "We must standardize and convert all audio to the same sampling rate so that all arrays have the same dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzKKGaWKjpsr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYsGZ7kPjrT3"
      },
      "source": [
        "##**Resize to the same length**\n",
        "Resize to get an equal-sized audio sample by extending duration by padding it with silence, or by truncating it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znnWj4IokMRx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCRK-RobkN3i"
      },
      "source": [
        "##**Data argumentation**\n",
        "Perform data augmentation on the raw audio signal by applying a Time Shift to shift the audio to the left or the right by a random amount. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsCJMa7Kkm_I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En16rWXzkneV"
      },
      "source": [
        "##**Feature extraction**: \n",
        "Speech recognition methods derive features from the audio, such as Spectrogram or Mel Frequency Cepstrum (MFCC).\n",
        "\n",
        "* Convert the augmented audio to a Mel Spectrogram.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41z2AiSYleGx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCyl8e_olgLh"
      },
      "source": [
        "**Acoustic modeling:** \n",
        "\n",
        "After features are extracted, these vectors are passed to acoustic models. An acoustic model attempts to map the audio signal to the basic units of speech such as phonemes or graphemes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3kBtRP8lsQA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJyV7EWznPsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}